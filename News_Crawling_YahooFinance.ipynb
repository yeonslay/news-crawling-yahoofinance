{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f59ca88-d7b4-4dcf-a3c7-9bd58f89fa61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba069604-967b-4fb1-9695-c34e35e1b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "# 0. 나스닥100 기업들의 티커\n",
    "def get_nasdaq100_tickers(yahoo_style: bool = False) -> list[str]:\n",
    "    \n",
    "    url = \"https://en.wikipedia.org/wiki/Nasdaq-100\"\n",
    "    html = requests.get(url, headers=HEADERS, timeout=20).text\n",
    "    tables = pd.read_html(html)\n",
    "\n",
    "    # 'Ticker' 또는 'Symbol' 컬럼이 포함된 표 찾기\n",
    "    for t in tables:\n",
    "        if any(col in t.columns for col in [\"Ticker\", \"Symbol\"]):\n",
    "            df = t\n",
    "            break\n",
    "    else:\n",
    "        raise RuntimeError(\"위키피디아에서 티커 표를 찾지 못했습니다.\")\n",
    "\n",
    "    col = \"Ticker\" if \"Ticker\" in df.columns else \"Symbol\"\n",
    "    tickers = df[col].dropna().astype(str).tolist()\n",
    "\n",
    "    # yahoofinance에서 검색가능한 형태로 형식을 바꿔줌\n",
    "    if yahoo_style:\n",
    "        tickers = [t.replace(\".\", \"-\") for t in tickers]\n",
    "\n",
    "    return tickers\n",
    "\n",
    "nasdaq100 = get_nasdaq100_tickers(yahoo_style=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a94eae-5630-4034-8249-734866ecd0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 티커에 대한 뉴스 url 요청\n",
    "def get_href(query, count=20):\n",
    "    url = f\"https://query1.finance.yahoo.com/v1/finance/search?q={query}&newsCount={count}&start=0\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                      \"Chrome/127.0.0.0 Safari/537.36\",\n",
    "        \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Referer\": \"https://finance.yahoo.com/\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(f\"[{query}] Status Code: {response.status_code}\")\n",
    "\n",
    "    results = []\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        for item in data.get(\"news\", []):\n",
    "            link = item.get(\"link\")\n",
    "            results.append(link)\n",
    "\n",
    "    return results[:count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8421e5f2-90d7-4925-a51e-3e55a3882c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. CSV 업데이트 함수\n",
    "\n",
    "def update_csv(query, csv_path=\"yahoo_news.csv\", filtered_path=\"yahoo_filtered.csv\"):\n",
    "    new_links = get_href(query)\n",
    "\n",
    "    # 이전에 크롤링한 data\n",
    "    if os.path.exists(csv_path):\n",
    "        df_old = pd.read_csv(csv_path)\n",
    "        old_pairs = set(zip(df_old[\"ticker\"], df_old[\"link\"]))\n",
    "    else:\n",
    "        df_old = pd.DataFrame(columns=[\"ticker\", \"link\", \"headline\", \"pubdate\", \"related_tickers\", \"article\"])\n",
    "        old_pairs=set()\n",
    "    \n",
    "    # 추출 대상에서 제외된 url data\n",
    "    if os.path.exists(filtered_path):\n",
    "        df_filtered = pd.read_csv(filtered_path)\n",
    "        filtered_pairs = set(zip(df_filtered[\"ticker\"], df_filtered[\"link\"]))\n",
    "    else:\n",
    "        filtered_pairs = set()\n",
    "\n",
    "    # 이전에 실행하지 않았던 url만 모으기\n",
    "    unique_links = [\n",
    "        link for link in new_links\n",
    "        if (query, link) not in old_pairs and (query, link) not in filtered_pairs\n",
    "    ]\n",
    "\n",
    "    # 새로운 데이터프레임 생성 (티커 포함)\n",
    "    df_new = pd.DataFrame(unique_links, columns=[\"link\"])\n",
    "    df_new[\"ticker\"] = query\n",
    "\n",
    "    # 합치기\n",
    "    df_updated = pd.concat([df_old, df_new], ignore_index=True)\n",
    "\n",
    "    # 중복 제거(더블 체크)\n",
    "    df_updated = df_updated.drop_duplicates(subset=[\"ticker\",\"link\"], keep=\"first\")\n",
    "\n",
    "    # 저장\n",
    "    df_updated.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(f\"[{query}] 새로운 링크 {len(unique_links)}개 추가됨. 전체 {len(df_updated)}개 저장됨.\")\n",
    "    return df_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798d2fbe-bb51-46e0-ab60-a16b3cacfb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 추출 대상에서 제외된 url을 csv로 저장해두는 함수\n",
    "def save_filtered(query, link, filtered_path=\"yahoo_filtered.csv\"):\n",
    "    # 기존 파일 불러오기\n",
    "    if os.path.exists(filtered_path):\n",
    "        df_filtered = pd.read_csv(filtered_path)\n",
    "    else:\n",
    "        df_filtered = pd.DataFrame(columns=[\"ticker\", \"link\"])\n",
    "\n",
    "    # 새로운 행 추가\n",
    "    new_row = pd.DataFrame([{\"ticker\": query, \"link\": link}])\n",
    "    df_filtered = pd.concat([df_filtered, new_row], ignore_index=True)\n",
    "\n",
    "    # 중복 제거\n",
    "    df_filtered = df_filtered.drop_duplicates(subset=[\"ticker\", \"link\"], keep=\"first\")\n",
    "\n",
    "    # 저장\n",
    "    df_filtered.to_csv(filtered_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"추출 대상에서 제외된 url 저장: {link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaad719-819e-4caa-a53f-50ce6cd7096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 기사 크롤링 함수\n",
    "def scrape_articles(df, query, driver, csv_path=\"yahoo_news.csv\"):\n",
    "\n",
    "    # 재실행 시 index 불일치 해결을 위함\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        link = df.iloc[i][\"link\"]\n",
    "\n",
    "        # 이미 headline 있으면 skip\n",
    "        if \"headline\" in df.columns and pd.notnull(df.iloc[i][\"headline\"]):\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        driver.get(link)\n",
    "        time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            # 필터링 조건1: PREMIUM News\n",
    "            head_str = driver.find_element(By.XPATH, '//*[@id=\"main-content-wrapper\"]')\n",
    "            is_premium = head_str.text.split('\\n', 1)[0].strip()\n",
    "            if is_premium == \"PREMIUM\":\n",
    "                print(f\"Skip PREMIUM article: {link}\")\n",
    "                save_filtered(query, link)\n",
    "                df = df.drop(df.index[i]).reset_index(drop=True)\n",
    "                continue\n",
    "            # 필터링 조건2: Yahoo Finance Video\n",
    "            try:\n",
    "                head_str_2 = driver.find_element(By.CLASS_NAME, 'byline-attr-author.yf-1k5w6kz')\n",
    "                if 'Yahoo Finance Video' in head_str_2.text:\n",
    "                    print(f\"Skip Yahoo Finance VIDEO article: {link}\")\n",
    "                    save_filtered(query, link)\n",
    "                    df = df.drop(df.index[i]).reset_index(drop=True)\n",
    "                    continue\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "\n",
    "            # 필터링 조건3: 다양한 출처들(개인적인 견해를 포함한 글)\n",
    "            try:\n",
    "                element = driver.find_element(\n",
    "                    By.CSS_SELECTOR,\n",
    "                    'div.cover-wrap div.top-header a.subtle-link[aria-label]'\n",
    "                )\n",
    "                aria_label_value = element.get_attribute('aria-label')\n",
    "                blocked_sources = [\n",
    "                    'Motley Fool', 'Barchart', 'The Wall Street Journal',\n",
    "                    'Zacks', 'Benzinga', 'Quartz'\n",
    "                ]\n",
    "                if any(src in aria_label_value for src in blocked_sources):\n",
    "                    print(f\"Skip SOURCE article ({aria_label_value}): {link}\")\n",
    "                    save_filtered(query, link)\n",
    "                    df = df.drop(df.index[i]).reset_index(drop=True)\n",
    "                    continue\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "\n",
    "            # 기사 제목\n",
    "            headline = driver.find_element(By.CLASS_NAME, 'cover-headline.yf-1rjrr1').text\n",
    "            # 발행일\n",
    "            pubdate = driver.find_element(By.CLASS_NAME, 'byline-attr-meta-time').text\n",
    "            # 관련 티커 (없는 경우 존재 -> None)\n",
    "            try:\n",
    "                tickers = driver.find_element(By.CLASS_NAME, 'carousel-top').text\n",
    "            except NoSuchElementException:\n",
    "                tickers = None\n",
    "            # 기사 본문\n",
    "            article = driver.find_element(By.CLASS_NAME, 'bodyItems-wrapper').text\n",
    "\n",
    "            # 더보기 버튼 존재 시 클릭 -> 추가 본문\n",
    "            try:\n",
    "                continue_button = driver.find_element(By.CSS_SELECTOR, \"button[aria-label='Story Continues']\")\n",
    "                continue_button.click()\n",
    "                time.sleep(1)\n",
    "                # 추가 본문\n",
    "                add_article = driver.find_element(By.CLASS_NAME, 'read-more-wrapper').text\n",
    "                # 본문 합치기\n",
    "                article += \"\\n\" + add_article\n",
    "            except NoSuchElementException:\n",
    "                pass\n",
    "\n",
    "            # df 업데이트\n",
    "            df.loc[i, \"ticker\"] = query\n",
    "            df.loc[i, \"headline\"] = headline\n",
    "            df.loc[i, \"pubdate\"] = pubdate\n",
    "            df.loc[i, \"related_tickers\"] = tickers\n",
    "            df.loc[i, \"article\"] = article\n",
    "\n",
    "            # 정상 처리됐으면 i 증가\n",
    "            i += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error on {link}: {e}\")\n",
    "            # 에러 난 행도 삭제 (다시 시도하지 않음)\n",
    "            df = df.drop(df.index[i]).reset_index(drop=True)\n",
    "            # i 증가 안 함 → drop했으니 다음 행이 자동으로 i 위치에 옴\n",
    "\n",
    "    # 저장 후 종료\n",
    "    df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"[{query}] 크롤링 완료 후 저장됨.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb54593a-4e93-4536-b09c-b287d47e7fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 결측치 제거 함수\n",
    "def cleanup_csv(csv_path=\"yahoo_news.csv\"):\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        mask = df[[\"headline\", \"pubdate\", \"article\"]].isnull().all(axis=1)\n",
    "        df = df.drop(df.index[mask]).reset_index(drop=True)\n",
    "        df.to_csv(csv_path, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"Null 행 {mask.sum()}개 삭제 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669356cb-d880-45bf-bf42-b7abfad219d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 모든 티커에 대해서 크롤링 실행\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 실행 전 클린업 (링크만 있고 기사 데이터 없는 행 제거)\n",
    "    csv_path = \"yahoo_news.csv\"\n",
    "    cleanup_csv(csv_path)\n",
    "\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    nasdaq100 = get_nasdaq100_tickers(yahoo_style=True)\n",
    "\n",
    "    try:\n",
    "        for ticker in nasdaq100:\n",
    "            df = update_csv(ticker, csv_path=csv_path, filtered_path=\"yahoo_filtered.csv\")\n",
    "            scrape_articles(df, ticker, driver, csv_path=csv_path)\n",
    "            time.sleep(random.uniform(3, 6))  # 과부하 방지\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        print(\"모든 크롤링 완료 후 브라우저 종료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a20f3c3-21fa-4243-988b-55cda425c0b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
